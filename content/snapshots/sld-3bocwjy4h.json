{"i":{"3bocwjy4h":{"id":"3bocwjy4h","type":"slide","setup":{"transparentBg":{"selectedKey":false},"bord":{"size":0,"rad":0},"bg":{"selectedKey":"image","fill":{"opac":1,"rgb":"var(--TC3)"},"img":{"filt":{"opac":100,"gray":0,"blur":0,"tint":{"rgb":"var(--TC3)","opac":0.5,"blendMode":"multiply"}},"pos":{"type":"crop","fit":{"bg":{"color":{"rgb":"#FFF","opac":100}},"vali":"center","hali":"center"},"crop":{"poi":{"x":557,"y":450}}},"measures":{"size":{"width":1080,"height":591}}}}}},"Nr6ZyNF0HB":{"id":"Nr6ZyNF0HB","type":"interactive","subtype":"interactiveRichText","setup":{"frmeTp":{"selectedKey":"bottomLine"},"bg":{"selectedKey":"fill","fill":{"opac":0.2,"rgb":"#FFF"}},"txt":{"val":{"ops":[{"insert":"Boosting","attributes":{"bold":true,"lang":"ES-US","color":"#124386","background":"rgba(0, 0, 0, 0)"}},{"insert":"\n","attributes":{"align":"center","size":"largeTitle"}}]},"rType":"richtext"},"vali":"middle","hali":"","frmeColo":{"color":{"opac":1,"rgb":"#124386"}}}},"FoT6unR5Vr":{"id":"FoT6unR5Vr","type":"interactive","subtype":"interactiveRichText","setup":{"frmeTp":{"selectedKey":"rightLine"},"bg":{"selectedKey":"fill","fill":{"opac":0,"rgb":"var(--TC14)"}},"txt":{"val":{"ops":[{"insert":"A diferencia del bagging, boosting se centra en mejorar la precisión de los modelos base secuencialmente. En lugar de entrenar modelos independientes, cada nuevo modelo en boosting se enfoca en corregir los errores cometidos por los modelos anteriores. AdaBoost y Gradient Boosting son dos técnicas populares de boosting.","attributes":{"lang":"ES-US","color":"var(--TC13)"}},{"insert":" ","attributes":{"color":"var(--TC13)"}},{"insert":"\n","attributes":{"align":"justify"}},{"insert":" ","attributes":{"color":"var(--TC13)"}},{"insert":"\n","attributes":{"align":"justify"}},{"insert":"AdaBoost, que significa \"Adaptive Boosting\", es un algoritmo de ensamble utilizado en problemas de clasificación y regresión. Fue propuesto por Freund y Schapire en 1996.","attributes":{"lang":"ES-US","color":"var(--TC13)"}},{"insert":"\n","attributes":{"align":"justify"}}]},"rType":"richtext"},"vali":"middle","paddV1":68,"hali":"","frmeColo":{"color":{"opac":1,"rgb":"#124386"}}}},"-njPVrlxSY":{"id":"-njPVrlxSY","type":"interactive","subtype":"interactiveRichText","setup":{"frmeTp":{"selectedKey":"noneFrame"},"bg":{"selectedKey":"fill","fill":{"opac":0.2,"rgb":"#FFF"}},"txt":{"val":{"ops":[{"insert":"La idea principal detrás de AdaBoost es construir un clasificador fuerte a partir de varios clasificadores débiles. Aquí hay algunos puntos clave sobre AdaBoost:","attributes":{"color":"#000000"}},{"insert":"\n\n","attributes":{"align":"justify"}},{"insert":"https://view.genial.ly/65e651be5130350014d495f5","attributes":{"color":"#0000ff","link":"https://view.genial.ly/65e651be5130350014d495f5"}},{"insert":"\n","attributes":{"align":"justify"}}]},"rType":"richtext"},"hali":"","paddV1":57}}},"v":"1"}